{
  "task_id": "BigCodeBench/1013",
  "instruct_prompt": "This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\nThe function should output with:\n    int: The number of unique absolute links scraped from the webpage.\nYou should write self-contained code starting with:\n```\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n```",
  "llm_response": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    Scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n\n    Args:\n    url (str): The URL of the webpage to scrape.\n    base_url (str): The base URL to use for relative links. Defaults to \"https://www.example.com\".\n    csv_file (str): The filename to save the scraped links to. Defaults to \"scraped_data.csv\".\n\n    Returns:\n    int: The number of unique absolute links scraped from the webpage.\n    \"\"\"\n\n    # Send a GET request to the webpage\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code != 200:\n        print(f\"Failed to retrieve webpage. Status code: {response.status_code}\")\n        return 0\n\n    # Parse the HTML content of the webpage\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # Find all hyperlinks on the webpage\n    links = soup.find_all('a', href=True)\n\n    # Initialize a set to store unique absolute links\n    absolute_links = set()\n\n    # Iterate over each link\n    for link in links:\n        # Get the href attribute of the link\n        href = link['href']\n\n        # Convert the link to an absolute URL\n        absolute_link = urljoin(base_url, href)\n\n        # Add the absolute link to the set\n        absolute_links.add(absolute_link)\n\n    # Write the absolute links to a CSV file\n    with open(csv_file, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Absolute Links\"])\n        for link in absolute_links:\n            writer.writerow([link])\n\n    # Return the number of unique absolute links\n    return len(absolute_links)\nurl = \"https://www.example.com\"\nbase_url = \"https://www.example.com\"\ncsv_file = \"scraped_data.csv\"",
  "model": "llama-3.3-70b"
}