{
  "task_id": "BigCodeBench/1013",
  "instruct_prompt": "This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\nThe function should output with:\n    int: The number of unique absolute links scraped from the webpage.\nYou should write self-contained code starting with:\n```\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n```",
  "llm_response": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    try:\n        # Send a GET request to the URL\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an error for bad responses\n    except requests.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return 0\n\n    # Parse the webpage content\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Find all hyperlinks\n    links = set()\n    for link in soup.find_all('a', href=True):\n        # Convert relative URLs to absolute URLs\n        absolute_url = urljoin(base_url, link['href'])\n        links.add(absolute_url)\n\n    # Write the unique links to a CSV file\n    try:\n        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            writer.writerow(['URL'])  # Write header\n            for link in links:\n                writer.writerow([link])\n    except IOError as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return 0\n\n    # Return the number of unique links\n    return len(links)",
  "model": "gpt-4o"
}