{
  "task_id": "BigCodeBench/723",
  "instruct_prompt": "Scrape data from a given URL and save the scraped data to a CSV file.\nThe function should output with:\n    CSV_FILE_PATH (str): The path of the CSV file where the scraped data is saved.\nYou should write self-contained code starting with:\n```\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n```",
  "llm_response": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrape data from a given URL and save the scraped data to a CSV file.\n    \n    Args:\n        url (str): The URL to scrape data from\n        \n    Returns:\n        str: The path of the CSV file where the scraped data is saved\n    \"\"\"\n    # Fetch the webpage content\n    response = urllib.request.urlopen(url)\n    html_content = response.read()\n    \n    # Parse the HTML content\n    soup = BeautifulSoup(html_content, 'html.parser')\n    \n    # Extract data - this is a generic example that extracts all links\n    # In a real-world scenario, you would customize this based on the specific website structure\n    data = []\n    \n    # Example: Extract all links and their text\n    links = soup.find_all('a', href=True)\n    for link in links:\n        data.append({\n            'text': link.get_text().strip(),\n            'url': link['href']\n        })\n    \n    # If no links found, try extracting all headings as alternative\n    if not data:\n        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n        for heading in headings:\n            data.append({\n                'tag': heading.name,\n                'text': heading.get_text().strip()\n            })\n    \n    # If still no data, extract all paragraphs\n    if not data:\n        paragraphs = soup.find_all('p')\n        for i, paragraph in enumerate(paragraphs):\n            text = paragraph.get_text().strip()\n            if text:  # Only add non-empty paragraphs\n                data.append({\n                    'type': 'paragraph',\n                    'text': text,\n                    'index': i\n                })\n    \n    # Write data to CSV file\n    if data:\n        # Get the fieldnames from the first record\n        fieldnames = data[0].keys()\n        \n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            writer.writerows(data)\n    else:\n        # Create an empty CSV with a default structure if no data found\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['No data found'])\n    \n    return CSV_FILE_PATH",
  "model": "qwen3-coder"
}