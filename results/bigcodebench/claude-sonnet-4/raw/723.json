{
  "task_id": "BigCodeBench/723",
  "instruct_prompt": "Scrape data from a given URL and save the scraped data to a CSV file.\nThe function should output with:\n    CSV_FILE_PATH (str): The path of the CSV file where the scraped data is saved.\nYou should write self-contained code starting with:\n```\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n```",
  "llm_response": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrape data from a given URL and save the scraped data to a CSV file.\n    \n    Args:\n        url (str): The URL to scrape data from\n        \n    Returns:\n        str: The path of the CSV file where the scraped data is saved\n    \"\"\"\n    try:\n        # Send HTTP request to the URL\n        with urllib.request.urlopen(url) as response:\n            html_content = response.read().decode('utf-8')\n        \n        # Parse HTML content with BeautifulSoup\n        soup = BeautifulSoup(html_content, 'html.parser')\n        \n        # Extract data - looking for tables first, then fallback to other elements\n        scraped_data = []\n        \n        # Try to find tables first (most structured data)\n        tables = soup.find_all('table')\n        if tables:\n            for table in tables:\n                rows = table.find_all('tr')\n                for row in rows:\n                    cells = row.find_all(['td', 'th'])\n                    if cells:\n                        row_data = [cell.get_text(strip=True) for cell in cells]\n                        scraped_data.append(row_data)\n        else:\n            # If no tables found, extract text from common elements\n            elements = soup.find_all(['p', 'div', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'])\n            for element in elements:\n                text = element.get_text(strip=True)\n                if text:  # Only add non-empty text\n                    scraped_data.append([text])\n        \n        # If still no data found, get all text content\n        if not scraped_data:\n            all_text = soup.get_text(strip=True)\n            if all_text:\n                # Split by lines and filter out empty lines\n                lines = [line.strip() for line in all_text.split('\\n') if line.strip()]\n                scraped_data = [[line] for line in lines[:50]]  # Limit to first 50 lines\n        \n        # Ensure we have at least some data structure\n        if not scraped_data:\n            scraped_data = [['No data found', 'URL: ' + url]]\n        \n        # Write data to CSV file\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            \n            # Write header if we have tabular data\n            if scraped_data and len(scraped_data[0]) > 1:\n                # Check if first row looks like headers\n                first_row = scraped_data[0]\n                if all(isinstance(cell, str) and len(cell) < 50 for cell in first_row):\n                    writer.writerow(['Column_' + str(i+1) for i in range(len(first_row))])\n            else:\n                writer.writerow(['Scraped_Content'])\n            \n            # Write all scraped data\n            for row in scraped_data:\n                writer.writerow(row)\n        \n        return CSV_FILE_PATH\n        \n    except urllib.error.URLError as e:\n        # Handle URL errors\n        error_data = [['Error', f'Failed to access URL: {url}', f'Error: {str(e)}']]\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Status', 'Message', 'Details'])\n            writer.writerows(error_data)\n        return CSV_FILE_PATH\n        \n    except Exception as e:\n        # Handle other errors\n        error_data = [['Error', f'Failed to scrape data from: {url}', f'Error: {str(e)}']]\n        with open(CSV_FILE_PATH, 'w', newline='', encoding='utf-8') as csvfile:\n            writer = csv.writer(csvfile)\n            writer.writerow(['Status', 'Message', 'Details'])\n            writer.writerows(error_data)\n        return CSV_FILE_PATH",
  "model": "claude-sonnet-4"
}