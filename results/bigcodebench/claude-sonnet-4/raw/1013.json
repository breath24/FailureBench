{
  "task_id": "BigCodeBench/1013",
  "instruct_prompt": "This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\nThe function should output with:\n    int: The number of unique absolute links scraped from the webpage.\nYou should write self-contained code starting with:\n```\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n```",
  "llm_response": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    Scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\n    \n    Args:\n        url (str): The URL of the webpage to scrape\n        base_url (str): The base URL to use for converting relative URLs to absolute URLs\n        csv_file (str): The filename for the CSV file to save the links\n    \n    Returns:\n        int: The number of unique absolute links scraped from the webpage\n    \"\"\"\n    try:\n        # Send GET request to the webpage\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for bad status codes\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Find all anchor tags with href attributes\n        links = soup.find_all('a', href=True)\n        \n        # Set to store unique absolute URLs\n        absolute_urls = set()\n        \n        # Process each link\n        for link in links:\n            href = link['href']\n            # Convert relative URLs to absolute URLs\n            absolute_url = urljoin(base_url, href)\n            absolute_urls.add(absolute_url)\n        \n        # Write the unique absolute URLs to CSV file\n        with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n            # Write header\n            writer.writerow(['URL'])\n            # Write each unique URL\n            for absolute_url in sorted(absolute_urls):\n                writer.writerow([absolute_url])\n        \n        return len(absolute_urls)\n    \n    except requests.RequestException as e:\n        print(f\"Error fetching the webpage: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0",
  "model": "claude-sonnet-4"
}