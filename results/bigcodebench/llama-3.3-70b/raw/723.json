{
  "task_id": "BigCodeBench/723",
  "instruct_prompt": "Scrape data from a given URL and save the scraped data to a CSV file.\nThe function should output with:\n    CSV_FILE_PATH (str): The path of the CSV file where the scraped data is saved.\nYou should write self-contained code starting with:\n```\nimport urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\n# Constants\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n```",
  "llm_response": "import urllib.request\nfrom bs4 import BeautifulSoup\nimport csv\nimport os\nCSV_FILE_PATH = 'scraped_data.csv'\ndef task_func(url):\n    \"\"\"\n    Scrapes data from a given URL and saves the scraped data to a CSV file.\n\n    Args:\n        url (str): The URL to scrape data from.\n\n    Returns:\n        str: The path of the CSV file where the scraped data is saved.\n    \"\"\"\n    try:\n        # Send a GET request to the URL\n        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n        response = urllib.request.urlopen(req)\n\n        # If the GET request is successful, the status code will be 200\n        if response.getcode() == 200:\n            # Get the content of the response\n            page_content = response.read()\n\n            # Create a BeautifulSoup object and specify the parser\n            soup = BeautifulSoup(page_content, 'html.parser')\n\n            # Find all the table rows on the webpage\n            table_rows = soup.find_all('tr')\n\n            # Create a list to store the scraped data\n            scraped_data = []\n\n            # Iterate over each table row\n            for row in table_rows:\n                # Find all the table data in the row\n                table_data = row.find_all('td')\n\n                # Create a list to store the data in the current row\n                row_data = []\n\n                # Iterate over each table data\n                for data in table_data:\n                    # Get the text of the table data and strip any leading or trailing whitespace\n                    text = data.get_text().strip()\n\n                    # Add the text to the row data list\n                    row_data.append(text)\n\n                # Add the row data to the scraped data list\n                scraped_data.append(row_data)\n\n            # Open the CSV file in write mode\n            with open(CSV_FILE_PATH, 'w', newline='') as file:\n                # Create a CSV writer\n                writer = csv.writer(file)\n\n                # Write each row of scraped data to the CSV file\n                writer.writerows(scraped_data)\n\n            # Return the path of the CSV file\n            return CSV_FILE_PATH\n\n        else:\n            # If the GET request is not successful, return an error message\n            return \"Failed to retrieve the webpage. Status code: {}\".format(response.getcode())\n\n    except Exception as e:\n        # If an exception occurs, return an error message\n        return \"An error occurred: {}\".format(str(e))\nurl = \"http://example.com\"",
  "model": "llama-3.3-70b"
}