{
  "task_id": "BigCodeBench/657",
  "instruct_prompt": "Generate word vectors from a list of texts using the gensim Word2Vec model and nltk.corpus.stopwords. The texts are first cleaned by removing all non-alphanumeric characters except space, lowercased, and stop words are removed.\nThe function should output with:\n    Word2Vec: A trained Word2Vec model.\nYou should write self-contained code starting with:\n```\nimport re\nimport nltk\nfrom gensim.models import Word2Vec\n# Constants\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords=None):\n```",
  "llm_response": "import re\nimport nltk\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nALPHANUMERIC = re.compile('[\\W_]+')\ndef task_func(texts, stopwords_list=None):\n    if stopwords_list is None:\n        stopwords_list = set(stopwords.words('english'))\n    \n    # Function to clean and tokenize text\n    def clean_text(text):\n        # Remove non-alphanumeric characters\n        text = ALPHANUMERIC.sub(' ', text)\n        # Lowercase the text\n        text = text.lower()\n        # Tokenize the text\n        tokens = word_tokenize(text)\n        # Remove stopwords\n        tokens = [word for word in tokens if word not in stopwords_list]\n        return tokens\n    \n    # Clean and tokenize all texts\n    tokenized_texts = [clean_text(text) for text in texts]\n    \n    # Train Word2Vec model\n    model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    return model\ntexts = [\n    \"This is a sample sentence.\",\n    \"Word2Vec is a great tool for natural language processing.\",\n    \"Let's see how it works with this example.\"\n]",
  "model": "gpt-4o"
}