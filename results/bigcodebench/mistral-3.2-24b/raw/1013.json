{
  "task_id": "BigCodeBench/1013",
  "instruct_prompt": "This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\nThe function should output with:\n    int: The number of unique absolute links scraped from the webpage.\nYou should write self-contained code starting with:\n```\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n```",
  "llm_response": "import requests\nfrom urllib.parse import urljoin, urlparse\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    \"\"\"\n    Scrapes a webpage for all hyperlinks, converts them to absolute URLs,\n    saves them to a CSV file, and returns the count of unique links.\n\n    Args:\n        url: The URL of the webpage to scrape\n        base_url: The base URL to use for resolving relative URLs\n        csv_file: The path to the CSV file where links will be saved\n\n    Returns:\n        int: The number of unique absolute links scraped from the webpage\n    \"\"\"\n    try:\n        # Fetch the webpage content\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Parse the HTML content\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        # Extract all links\n        links = set()  # Using a set to automatically handle duplicates\n        for a_tag in soup.find_all('a', href=True):\n            href = a_tag['href'].strip()\n\n            # Skip empty links and javascript links\n            if not href or href.startswith('javascript:'):\n                continue\n\n            # Convert to absolute URL\n            absolute_url = urljoin(base_url, href)\n\n            # Validate the URL\n            parsed = urlparse(absolute_url)\n            if parsed.scheme in ('http', 'https') and parsed.netloc:\n                links.add(absolute_url)\n\n        # Save links to CSV\n        with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n            writer = csv.writer(f)\n            writer.writerow(['absolute_url'])  # Write header\n            for link in sorted(links):\n                writer.writerow([link])\n\n        return len(links)\n\n    except requests.RequestException as e:\n        print(f\"Error fetching the URL: {e}\")\n        return 0\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return 0",
  "model": "mistral-3.2-24b"
}