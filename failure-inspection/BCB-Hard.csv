task_id,failure_analysis,category
BigCodeBench/101,"The task is to generate a Seaborn correlation heatmap of the Boston Housing dataset and return the plot object, with an option to save the figure. Across models, failures stemmed from two consistent mismatches with the task and tests: correctly parsing the Boston dataset’s unusual two-line format and restricting the heatmap to the 13 feature columns (no target) with no titles or labels. deepseek-v3, mistral-3-2, and qwen3-coder misread the file layout as a simple table, so they couldn’t even build the right data; gpt-4o got close but mislabeled the columns to include a target, which broke shapes and would also have spoiled the expected 13×13 correlations; llama-3-3 and sonnet-4 ignored the provided URL and relied on a removed external loader, so they failed early and, even if they hadn’t, they planned to include the target and add a title. Several models (e.g., deepseek-v3, gpt-4o, sonnet-4) also added plot titles or masks that conflict with the tests, which expect a full 13×13 correlation matrix and empty titles/labels. In short, small divergences from the prompt’s constraints and the tests’ strict expectations—especially the file’s structure, excluding the target, and leaving the plot unlabeled—cascaded into consistent failures across models.","Flawed/Incomplete Algorithm, Wrong Problem Mapping"
BigCodeBench/1012,"The task is to download a ZIP file from a given URL, extract its contents, and return a status message with the list of extracted files. Across models, most failures stem from not honoring the task’s strict message contract and the tests’ expectations: deepseek-v3, gpt-4o, mistral-3-2, and qwen3-coder return a generic “Success” instead of the required “Download and extraction successful,” while gpt-4o, qwen3-coder, sonnet-4, llama-3-3, and mistral-3-2 mishandle non-200 responses, producing generic errors (or unzip errors) rather than the mandated “Download failed …”. Llama-3-3 gets the success message right but still ignores the non-200 branch; mistral-3-2 also shows brittle cleanup that crashes under network errors. These behaviors suggest a common misunderstanding: treating all bad outcomes as generic exceptions instead of following the specific status messaging rules called out in the prompt and implicitly enforced by the tests. The test design (mocking only the HTTP status and not fully emulating response content/raise-for-status) intentionally pressures solutions to check status codes explicitly, exposing approaches (e.g., gpt-4o, qwen3-coder, sonnet-4) that proceed as if a normal response exists. Overall, the prompt’s phrasing requirements and the tests’ targeted assertions are clear but unforgiving; models that didn’t align their messages precisely—or that failed to separate HTTP failures from extraction errors—fell short.","Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/1015,"The task is to parse an HTML table from either a webpage or local file and store its contents into an SQLite database, returning the number of rows extracted. Across submissions, most failures trace to two themes: brittle table parsing choices and mishandling of file:// inputs under the test setup. deepseek-v3, gpt-4o, llama-3-3, and mistral-3-2 all leaned on pandas’ HTML parser, which in this environment requires a newer BeautifulSoup than is installed, so they crashed on an import error rather than gracefully handling empty or simple tables. gpt-4o and llama-3-3 also treated file:///… like a web URL, triggering “no connection adapter” or file-load errors; sonnet-4 tried to detect local files but then rejected the mocked file:// path before reading it; qwen3-coder misclassified file:// as a URL and also surfaced parsing errors as database errors. Several models (qwen3-coder, sonnet-4) returned the wrong exception type, and a few included a stray stubbed function definition, hinting at rushed assembly. The prompt and tests introduce subtle traps: local files are provided only as file:// URIs, HTTP responses are mocked via the .content attribute (not .text), and the environment intentionally makes pandas’ HTML helper unreliable—so approaches that depend on those assumptions fail consistently.",Flawed/Incomplete Algorithm
BigCodeBench/1019,"The task is to extract text from an image using OCR, convert it between encodings, and if OCR fails, fall back to the image’s embedded comment. Across models, most failures stem from mishandling the task’s control flow and error policy rather than OCR itself: the prompt expects “try OCR → if nonempty convert → on conversion errors raise ValueError → else fall back to the image comment → finally return empty string,” and the tests enforce that with mocked images and comments. deepseek-v3 and llama-3-3 broadly catch/transform errors or misuse the fallback, so real encoding problems don’t surface as the required ValueError and valid comments get ignored. sonnet-4 suppresses the very ValueError the tests demand when OCR conversion is wrong, then fails to return the available comment. mistral-3-2 and qwen3-coder interact poorly with the mocked objects, so instead of real strings they bubble mock placeholders, which the assertions reject; qwen3-coder also muddles the notion of “convert from X to Y,” leading to inconsistent outputs. gpt-4o adds an unnecessary file-existence check, conflicting with the test harness that uses dummy paths, so everything aborts early. Overall, the common pattern is deviation from the specified “raise only on encoding-conversion errors, otherwise fall back or return empty,” plus fragile handling of the provided (mocked) metadata; the prompt and tests are consistent, but they assume no extra filesystem checks and precise, minimal exception behavior.",Flawed/Incomplete Algorithm
BigCodeBench/1039,"The task is to secure a client socket with SSL/TLS, receive a requested file path, and return the file’s SHA256 hash or an error message. Across models, most failures stem from small mismatches with strict tests rather than core functionality: deepseek-v3 and mistral-3-2 behaved reasonably but didn’t satisfy the test’s exact expectations for how a response must be sent and how the error text must be worded; gpt-4o also missed a required cleanup step during errors; llama-3-3 computed results but didn’t actually send a response back to the client; sonnet-4 returned clear errors yet failed to close the secure connection on exceptions; and qwen3-coder was closest to passing but lost points for the precise error string format. A recurring quirk is that several models emitted a redundant stub before the real function, hinting at generation noise. Overall, the prompt is straightforward, but the tests are unusually opinionated—they demand a specific send method, an exact error phrase, and guaranteed socket closing—so otherwise sensible approaches were marked wrong due to these tight, inflexible checks.","Flawed/Incomplete Algorithm, Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/1042,"The task is to receive a message from a client socket and forward it as an email through an SMTP server. Across all models, failures largely stem from breaking the prompt’s I/O contract and clashing with the tests’ expectations: most models (deepseek-v3, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4, and gpt-4o) asked for sender/recipient via input rather than getpass, which the harness doesn’t mock, causing immediate read errors and preventing any SMTP call, acknowledgment, or clean socket close; additionally, gpt-4o and sonnet-4 swallowed exceptions, so an injected SMTPConnectError never surfaced (the tests require it), and several didn’t send the required “Message sent.” response or guarantee closing the socket, leading to further assertion failures; finally, there’s a small spec/test mismatch—tests demand skipping email send on an empty message, while the provided canonical snippet doesn’t show that guard—so even near-correct approaches would still fail without that behavior.",Flawed/Incomplete Algorithm
BigCodeBench/1057,"The task is to create a pandas DataFrame showing all animal–food combinations in an ""animal:food"" format, using defaults or empty outputs depending on the inputs. All models failed for broadly the same reasons: they misread the special-case rules and used the wrong defaults. The prompt is misleading—it says returning an empty DataFrame when lists are “empty or not provided,” but the tests expect the opposite for “not provided” (use large defaults to get a 10×7 grid). Deepseek-v3, Llama-3-3, Mistral-3-2, and Qwen3-coder all treated “not provided” as empty, immediately producing empty outputs; GPT-4o and Sonnet-4 also replaced empty inputs with small ad-hoc defaults (3×3 or 5×5), so shapes never matched what the tests enforce. Several models (e.g., GPT-4o, Sonnet-4) computed shuffled combinations but didn’t actually rely on them, hinting at spec drift rather than precise requirement tracking. Sonnet-4 further showed a logic-ordering glitch that made its empty-input check ineffective, yielding the same wrong result for multiple scenarios. Across the board, the root cause is ambiguity between “None” vs [] and ignoring the exact default sizes implied by the tests; once those two points are off, every downstream assertion fails. In short, the failures reflect a prompt–test inconsistency that required prioritizing the tests, plus casual default choices by the models that guaranteed shape mismatches.",Flawed/Incomplete Algorithm
BigCodeBench/1077,"The task is to calculate the average time difference in seconds between consecutive timestamps in a list after converting them to a specified timezone. Across models, the failures mostly stem from ignoring the specified timestamp format and the task’s simple sequencing logic rather than any flaw in the prompt or tests. The docs clearly state inputs like “dd/mm/yy HH:MM:SS.fff” and ask for average absolute gaps after timezone conversion, yet gpt-4o, mistral-3-2, qwen3-coder, and sonnet-4 all assumed ISO timestamps, so they crashed on parsing; llama-3-3 also used the wrong pattern and would have produced incorrect gaps even if it had parsed; and deepseek-v3 silently skipped unparsed items, collapsing to 0.0 in every case. A few approaches (notably llama-3-3 and deepseek-v3) also conflated “assigning” a timezone with “converting” to one, which would distort results if parsing had succeeded. The test suite is coherent (including out-of-order inputs and day boundaries), and the only subtlety is that the reference treats inputs as UTC before conversion; however, this wasn’t the primary cause—the predominant issue was models overriding the stated input format and requirements.","Flawed/Incomplete Algorithm, Formatting Mistakes"
BigCodeBench/1129,"The task is to parse a JSON string, fetch a file from a URL inside it, and save the file locally with a timestamped name. The task is to parse a JSON string, fetch a file from a URL inside it, and save the file locally with a timestamped name. Overall, the models failed for predictable, test-harness reasons rather than core logic: the tests stub the network call with a very minimal mock, but several models (deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4) added “real-world” robustness (extra HTTP checks, streaming, etc.) that the mock didn’t support, causing crashes. A second recurring mismatch was error handling: the tests implicitly require a specific missing-key exception, while deepseek-v3 and llama-3-3 converted it to a different error type, so they failed even before file writing mattered. Sonnet-4 also chose a text-saving approach, which conflicts with the test’s expectation of raw bytes. There’s nothing fundamentally wrong with the prompt, but it doesn’t spell out these testing constraints; the testcases are strict in subtle ways (minimal mock, exact exception type, byte content), so “defensive” additions that would be sensible in production backfired here. A minor oddity is that several responses duplicated the function definition, which isn’t fatal but signals generation noise.",Flawed/Incomplete Algorithm
BigCodeBench/1137,"The task is to extract phone numbers from a webpage or local file and save them in a JSON file. Across models, most failures stem from a simple mismatch with the test setup: every input is a local file passed as a file:// URL, while several models either treated it as a normal path or as an HTTP link, so they read nothing and returned empty lists (gpt-4o, llama-3-3, mistral-3-2) or even crashed on “invalid schema” (qwen3-coder). deepseek-v3 went further by reformatting numbers instead of returning the raw strings the tests expect, and also skipped writing the JSON output on error, which triggered missing-file assertions; sonnet-4’s code was incomplete and returned None. The tests are straightforward but strict: they require recognizing file://, preserving the exact +... numbers in order, and always writing the JSON file—even when no numbers are found. Nothing is wrong with the prompt or testcases, but the subtle “local file via URL” requirement and “no reformatting” expectation tripped most approaches.",Flawed/Incomplete Algorithm
BigCodeBench/120,"The task is to generate a pandas Series of random dates within a given date range, ensuring reproducibility with a seed and proper input validation. All six models—deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4—behaved similarly and failed for the same basic reason: they interpreted “inclusive range” as meaning the series should contain an inclusive number of days, while the tests expect a shorter series length (based on the gap between the dates) but still allow both endpoints as possible values. This off-by-one view cascaded into two visible breakages: the length check (e.g., 10 vs 9 days for a short range) and the exact reproducibility list, where each model produced one extra date compared with the expected sequence. The prompt and doc text contribute to the confusion: phrases like “including both start_date and end_date” and “length matches the number of days in the specified range” are ambiguous and nudge models toward the inclusive count, whereas the tests and canonical solution quietly require a different notion of length. Some responses also oddly duplicated the function (a stub followed by a full version), which is unusual but not the cause of failure. In short, a subtle spec–test mismatch about “inclusivity” led every model to the same mistaken approach and identical test failures.",Wrong Problem Mapping
BigCodeBench/124,"The task is to validate a list, append a value, generate random numbers based on its sum, and plot their histogram while returning the runtime and plot object. All six models broadly followed the task but failed the tests for the same core reason: they inferred from the prompt’s “number range (1–100) on the x-axis” that the histogram should span one bin per integer, while the hidden expectation in the tests (and canonical) was a 20-bin histogram, making the seeded bar heights not match; this ambiguity between the prose and the tests is the main source of failure. DeepSeek-V3, GPT-4o, Llama-3.3, Mistral-3.2, Qwen3-Coder, and Sonnet-4 all stumbled on this, producing many more bars than expected and thus mismatched frequencies. Several also tweaked the title text—Mistral-3.2 (“…(1–100)”), Qwen3-Coder and Sonnet-4 (“Distribution of Random Numbers”)—which violated an exact-string check. Mistral-3.2 additionally timed the wrong portion of the process (not penalized by tests), and some responses contained duplicate function stubs or stray lines (e.g., Llama-3.3), hinting at generation hygiene issues. Overall, the failures reflect a reasonable but incorrect reading of the prompt, compounded by strict, implicit test assumptions (20 bins, exact title) that weren’t explicitly stated.","Wrong Problem Mapping, Formatting Mistakes"
BigCodeBench/17,"The task is to check if a process with a given name is running, and either start it if not found or restart it if it is already running. All models struggled on this task because they failed to align with how the tests were written and what they were checking. A central issue was that the tests simulated processes in a way that only worked if the solution used one specific method of checking the process name, but most models (such as Deepseek-v3, GPT-4o, Llama-3-3, Mistral-3-2, Qwen3-coder, and Sonnet-4) relied on a different approach, so they consistently misdetected running processes and returned the wrong message. Several models also added extra arguments when starting processes (for example, using lists or enabling shell mode), which caused them to fail because the tests required an exact match. In some cases, like Llama-3-3 and Deepseek-v3, the models only stopped the first matching process instead of handling all instances, which the tests explicitly checked for. Overall, the failures were less about misunderstanding the task description and more about mismatches between the models’ coding habits and the strict expectations of the test setup.","Flawed/Incomplete Algorithm, Formatting Mistakes"
BigCodeBench/177,"The task is to filter articles by keywords in their titles and then count word frequencies from their content while excluding punctuation. All six models—deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4—fail for the same two high-level reasons: they normalize the article content to lowercase before counting words (merging “Data” with “data”, “What” with “what”), and they raise an error when the DataFrame is empty. Both choices clash with what the tests and the reference expect: counts must preserve original casing, and an empty but well-formed DataFrame should simply yield an empty result. The lowercasing is a reasonable “text-cleaning” instinct but contradicts the task’s evaluation; the empty-DataFrame error stems from a prompt/test mismatch—the prompt says to raise on emptiness, while the tests (and canonical solution) do not. A few stylistic quirks (e.g., deepseek-v3’s duplicate function, qwen3-coder/sonnet-4’s stricter title patterns) don’t drive the failures. In short, the models generally read the instruction like a generic NLP cleanup task, but the hidden grading criteria hinge on preserving case and treating an empty dataset as a valid no-op, and the prompt itself contributes to confusion by being inconsistent with the tests.",Wrong Problem Mapping
BigCodeBench/208,"The task is to generate a random walk of given length, plot it, and return its descriptive statistics. In this task, all models struggled with small but important mismatches against the strict test expectations, showing how sensitive the setup is. Deepseek-v3, GPT-4o, and Mistral-3-2 produced correct-looking results but calculated statistics slightly differently, leading to incorrect standard deviations and, in Deepseek and Mistral’s case, also adding extra words to the plot title. Llama-3-3 relied on default statistical summaries and missed required percentiles, while Qwen3-coder and Sonnet-4 both inserted an extra starting point in the random walk, making their plots and counts too long and causing repeated failures. Some models also used different naming for percentiles or altered titles, which the strict tests did not allow. Overall, the prompt itself was clear, but the tests required exact formatting and values, so even minor deviations in naming, counting, or statistical conventions caused failures across all models.","Flawed/Incomplete Algorithm, Formatting Mistakes"
BigCodeBench/211,"The task is to download a ZIP file from a given URL, save it, extract its contents into a target folder, and return the list of extracted files. All models failed for largely the same reason: they tried to be “helpful” by creating the destination folder, which—given the tests’ absolute path (/path/to/destination) and lack of mocking for directory creation—triggered permission errors before anything else could be validated. Beyond that shared pitfall, several models (e.g., deepseek-v3, gpt-4o, mistral-3-2, qwen3-coder, sonnet-4) ignored the required default request header and/or hard-coded a download filename, clashing with tests that strictly expect a specific header and a name derived from the URL; some (gpt-4o, llama-3-3, mistral-3-2) even removed the ZIP, contradicting the prompt’s “keep” instruction, while deepseek-v3 extracted from memory and never saved the file. The task and tests contain subtle traps: they mock most I/O but not directory creation, enforce exact call signatures, and implicitly require keeping the archive—small mismatches that, cumulatively, caused every approach to fail.","Flawed/Incomplete Algorithm, Formatting Mistakes"
BigCodeBench/214,"The task is to generate a random RGB image with given parameters and return both the image and its plot. All six models—deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4—generated valid-looking images with the right shapes, types, and error checks, yet they all failed because their random images didn’t match the test’s hard-coded expected pixels. The shared behavior that led to this was choosing a “sensible” vectorized NumPy approach to draw random values, while the hidden expectation (revealed by the tests and reference) was a very specific sequence tied to Python’s standard random generator; thus, even with the same seed, the numbers differ and the equality check explodes. Several models also emitted duplicate function definitions (first a stub with pass, then the “real” one), which is odd but not the root cause. Plotting choices (e.g., using or skipping cv2.cvtColor, calling plt.show()) didn’t matter. The deeper issue is with the test design and prompt: the prompt never states the exact randomness method required, but the tests demand an exact byte-for-byte image, making the task brittle and punishing reasonable alternative implementations like those from gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4, and deepseek-v3.",Wrong Problem Mapping
BigCodeBench/227,"The task is to create a matrix from a list, normalize it using the sound pressure level of an audio file, and generate a spectrogram from it. All models failed primarily because they checked for the audio file’s existence in a way the tests didn’t mock—tests patch os.path.isfile, while the submissions (deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4) mostly used a different existence check, so they incorrectly raised FileNotFoundError and never reached the core logic. Beyond that shared trigger, several approaches would still have stumbled: gpt-4o and mistral-3-2 normalized the matrix in ways that don’t match the SPL-based scaling the tests implicitly expect; qwen3-coder padded/truncated input instead of letting reshape fail (the tests expect an error for empty input); llama-3-3 and sonnet-4 drew spectrograms without calling librosa.display.specshow, which the tests explicitly check; and some responses included extra, duplicate, or demo code. The prompt itself is clear, but the test harness has strict, somewhat hidden contracts (use isfile, call specshow, let reshape raise) that the models violated, so the mismatch between test design and model choices is the real reason everything toppled.",Wrong Problem Mapping
BigCodeBench/241,"The task is to take a list of labeled numeric data, normalize the numeric values, and plot the original and normalized results together. All six models—deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4—failed for the same core reasons: they treated the input as a plain list of numbers instead of a list of (label, value) tuples, which made them try to process strings like “a” and crash; they chose MinMax scaling rather than the L2 normalization the tests expect; and none handled the empty-input case, which triggered errors. A couple of presentation mismatches would have also tripped tests later (mistral-3-2 and sonnet-4 used label text that doesn’t match what the tests look for). Some responses showed sloppy generation (duplicate function stubs; llama-3-3 even appended stray example code), suggesting the models weren’t tightly following constraints. The prompt likely contributed to the confusion: it says “normalize” without specifying which kind, and it pre-imports scikit-learn, nudging models toward MinMaxScaler; meanwhile, the tests quietly enforce L2 normalization and require unzipping the tuples, so the instructions and validation signals weren’t perfectly aligned.",Wrong Problem Mapping
BigCodeBench/267,"The task was to modify a dictionary, generate a signal from its values, perform a Fast Fourier Transform (FFT), and plot the result with specific labels. All models (Deepseek-V3, GPT-4o, Llama-3-3, Mistral-3-2, Qwen3-Coder, and Sonnet-4) failed on this task for essentially the same reason: they followed the prompt’s instruction literally and titled the plot “FFT of the signal,” while the test cases expected “FFT of the Signal” with a capital “S.” This small mismatch in capitalization was enough to trigger failures in every case, showing how sensitive the evaluation is to exact string matches. In fact, most models also used slightly different axis labels than required, which would have caused further errors if the title check had not already failed first. The signal generation approaches varied widely—some models (like Llama-3-3 and Sonnet-4) tried to create more elaborate waveforms, while others (like GPT-4o and Deepseek-V3) just transformed the raw dictionary values—but because the tests only cared about the labels and return types, these differences did not matter. The main issue was not the models’ reasoning about signals but the inconsistency between the prompt (lowercase “signal”) and the test oracle (uppercase “Signal”), which misled all models into producing outputs that could never pass.",Formatting Mistakes
BigCodeBench/273,"The task is to create an HTTP POST request handler that validates incoming JSON data and returns success or specific error messages based on the request. Across models, failures stemmed from not following the task’s narrow contract and the tests’ expectations. Several (gpt-4o, llama-3-3, mistral-3-2) tried to be “helpful” by spinning up a real server instead of returning a handler class, so they didn’t match what the tests call for. Others (deepseek-v3, qwen3-coder, sonnet-4) chose a custom error-response style rather than the specific error mechanism the tests watch for, which also triggered mock-related crashes. A smaller but consistent mismatch was header handling: tests provide lowercase header names, while some models (gpt-4o, llama-3-3, mistral-3-2, sonnet-4) assumed standard capitalized forms, causing good requests to be treated as bad. None of these issues are about core logic; they’re mostly interface discipline and test-harness alignment. The prompt is clear but slightly brittle: it emphasizes exact error messages and implies lowercase header keys, and the test harness punishes deviations (e.g., logging attributes missing in mocks). In short, models failed by over-building or defaulting to typical patterns instead of adhering to the test’s strict, idiosyncratic protocol.","Wrong Problem Mapping, Formatting Mistakes"
BigCodeBench/274,"The task is to build an HTTP POST request handler that reads email data from JSON and sends it through an SMTP server while returning proper status codes for errors. Across models, most failures stemmed from mismatches with the test harness’s narrow expectations rather than from email logic itself. deepseek-v3, qwen3-coder, and sonnet-4 chose an email-sending approach the tests didn’t look for, so success and auth-failure checks didn’t trigger as intended. gpt-4o and mistral-3-2 tried to be “helpful” by adding headers and richer error output, which collided with a minimal harness that only expects simple status responses and lacks full server state, causing crashes. llama-3-3 simply returned nothing usable, so all tests failed at setup. Several models (qwen3-coder, sonnet-4) also reported errors via a different API than the tests assert on, creating systematic misses. A subtle prompt-test misalignment contributed: the prompt hints at content type and length and names exception types, which some models interpreted as reasons to add headers or re-raise errors, while the tests reward a minimal, quiet response style.",Formatting Mistakes
BigCodeBench/313,"The task is to organize files by reading their contents and moving each file into a subdirectory named after the first piece of text that appears before a square bracket. Across models, the central failure was a consistent misreading of the task: most (gpt-4o, sonnet-4, qwen3-coder, mistral-3-2, llama-3-3, deepseek-v3) extracted categories from the filenames rather than from the file contents, so they never produced keys like “example”/“sample” that the tests expect from the content prefix before the first “[”. This led to wrong groupings (e.g., grouping by “test_file1”), moving files that should stay put (empty files or content starting with a bracket), and even crashes—deepseek-v3 hit path collisions (trying to move into a file path treated as a folder) and llama-3-3 raised NoneType errors from fragile matching. The prompt’s wording (“first text that is not enclosed in square brackets”) is slightly ambiguous about whether to parse filenames or contents, but the provided tests and canonical solution clearly anchor the rule in file content; models that ignored this context systematically failed. A few also showed generative artifacts (duplicate function stubs), which didn’t help clarity but weren’t the primary cause.",Wrong Problem Mapping
BigCodeBench/324,"The task is to start a list of files as subprocesses at the same time and return their exit codes. All models failed for the same broad reason: they didn’t align their behavior with what the tests implicitly require—start processes concurrently and report the current status via a non-blocking check that can be None if still running. deepseek-v3, gpt-4o, and qwen3-coder treated the task as “run scripts to completion,” so they blocked and returned generic error codes instead of the expected mixture of codes and None. llama-3-3, mistral-3-2, and sonnet-4 did start subprocesses but then waited for completion and read values the tests weren’t controlling, leading to mocked placeholders instead of real codes. Several responses also hard-wired how to launch files (e.g., assuming Python) or duplicated function stubs—signs of drifting from the minimal contract. The prompt is brief and the tests encode the true spec (concurrent start + status polling), but that spec isn’t spelled out explicitly; this mismatch nudged models toward “finish then report” logic, which can never produce the “still running” outcome the tests expect.","Flawed/Incomplete Algorithm, Wrong Problem Mapping"
BigCodeBench/326,"The task is to find all .bat files in a directory, run them, and return their names with the exit codes. All models failed for broadly the same reason: they added “helpful” behaviors that conflicted with the test setup’s expectations. deepseek-v3 and gpt-4o rejected the input outright by insisting the directory be real, so every test crashed; llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4 also gated on directory checks and simply returned nothing, ignoring the mocked files the tests supply. Several models (notably gpt-4o, mistral-3-2, qwen3-coder, sonnet-4) chose alternative ways to run commands that the tests don’t simulate, so the results couldn’t be controlled by the harness. sonnet-4 further assumed non-Windows systems shouldn’t run batch files, which clashes with the OS-agnostic, fully mocked tests. The prompt itself is slightly ambiguous (it doesn’t forbid path validation or platform checks), but the tests implicitly require proceeding regardless of real filesystem/OS, so “defensive” additions and platform heuristics—while reasonable in the wild—became the main source of failure here.",Flawed/Incomplete Algorithm
BigCodeBench/341,"The task is to create a function that takes a DataFrame column and produces a figure with two plots showing the distribution of its values. All models failed for the same underlying reason: they did not handle the categorical case in the way the tests expected. The prompt itself was slightly misleading, since it asked for a “box plot” as the second subplot regardless of data type, while the hidden tests actually required a scatter-style plot for categorical data. Models like GPT-4o, LLaMA-3-3, Mistral-3-2, and Sonnet-4 followed the instruction literally and always used a box plot, which caused the categorical test to fail. DeepSeek-V3 tried to avoid plotting for categorical values and instead left the subplot empty, which also failed. Qwen3-Coder attempted to adapt but produced another bar plot for the second subplot, which again did not match the test’s expectations. In short, the failures were not due to models ignoring the task, but rather due to a mismatch between the prompt’s wording and the grader’s hidden requirement, leading every model to produce results that looked reasonable but still failed the categorical check.",Formatting Mistakes
BigCodeBench/346,"The task is to run a Python script as a subprocess with given arguments, returning its exit code or raising an error if the script itself raises an exception. All models struggled on this task mainly because the prompt’s requirement to raise an error only when the script raises an exception is unusual and conflicts with the more common practice of treating any non-zero exit code as a failure. Deepseek-v3, GPT-4o, Llama-3-3, Mistral-3-2, and Sonnet-4 all fell into this trap, raising errors when a script exited with code 1, even though the tests expected the return code to be passed back normally. Qwen3-coder made the opposite mistake, never raising an error at all, so it failed when a script truly raised an exception. Mistral-3-2 also introduced an extra bug by misusing the error type, which caused additional crashes. These failures show a shared misunderstanding of the task’s subtle distinction between a Python exception in the child process and an ordinary non-zero exit, a distinction that the test cases make clear but the prompt itself leaves somewhat ambiguous.",Wrong Problem Mapping
BigCodeBench/360,"The task is to read data from an Excel sheet, calculate each column’s mean and standard deviation, and return both the results and a bar chart visualization. All models failed for broadly similar reasons: they made reasonable but test-incompatible choices under an underspecified prompt, then hit strict checks. The biggest trap was the standard deviation: the tests silently expect population values (10.0) while the prompt never clarifies this, so llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4 computed the more common sample values (~14.14) and were marked wrong. Several also returned the right numbers in the wrong shape or with the wrong key names—gpt-4o used std_dev instead of std, and deepseek-v3 reorganized the dictionary under top-level ""mean""/""std""—which failed exact-equality assertions. Visually, many (e.g., gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4) plotted means and standard deviations as two separate bars per column, but the tests assume only mean bars (with std as error bars), causing figure checks to fail. Deepseek-v3 also showed duplicate function stubs, an oddity but not the core issue. Overall, the failures stem from a mix of minor schema mismatches and an ambiguous prompt (population vs sample std, bar chart semantics) colliding with rigid test expectations the models couldn’t see.","Flawed/Incomplete Algorithm, Formatting Mistakes"
BigCodeBench/374,"The task is to process all Excel files in a directory and protect double quotes inside cells by adding backslashes, then return how many files were handled. In this task most models showed consistent misunderstandings of the requirements, leading to similar types of failures. A common issue was confusing “processed files” with “modified files”: for example, deepseek-v3 and gpt-4o only counted files where changes were made, so they returned too small a number. Several models, including qwen3-coder, sonnet-4, llama-3-3, and deepseek-v3, did not raise the required error when the directory was missing, while mistral-3-2 even tried to create the directory instead, which broke the tests. Quote escaping also caused trouble: sonnet-4 over-escaped and added too many backslashes, while others like llama-3-3 failed by misusing the spreadsheet library and crashing. These failures show that the prompt’s wording around “double backslashes” was easy to misinterpret, and the test cases exposed subtle differences between “files found” versus “files changed,” as well as the precise error-handling expected. Overall, the errors arose less from missing logic and more from small but crucial mismatches in how the instructions were interpreted.","Flawed/Incomplete Algorithm, Edge Case Mishandling"
BigCodeBench/399,"The task is to create a function that plots sine and cosine waves for a given frequency and sample size, returning the plot objects while handling invalid inputs. All models in this task (deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4) produced generally correct solutions that validated inputs, plotted sine and cosine waves, and returned the expected objects, but they all failed the tests for the same reason: the plotted lines were not labeled exactly as 'sin' and 'cos'. Instead, models tended to use more descriptive or human-friendly labels such as “Sine Wave” or “Cosine Wave (f=...)”. This points to a mismatch between the hidden evaluation criteria and the instructions in the prompt, which never stated the need for strict label names. Several models also showed duplicated function definitions (first a placeholder, then the full version), but this did not affect the outcome. Overall, the failures were not due to misunderstanding the mathematical or plotting requirements, but rather to an unstated and brittle test expectation that the models could not have inferred from the prompt alone.",Formatting Mistakes
BigCodeBench/409,"The task is to read an Excel file, extract a specific column, and return its mean, median, and standard deviation in a dictionary. Across the board, the models failed mainly because of mismatches between their outputs and the strict expectations of the tests. Most models (deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4) returned the results using dictionary keys like “std” or “standard_deviation” instead of the required “std_dev”, which caused immediate errors. Several of them (deepseek-v3, gpt-4o, mistral-3-2, qwen3-coder, sonnet-4) also used a different definition of standard deviation than the one expected, so even if the key names were correct their numeric answers would not match. Sonnet-4 showed an additional failure: it raised the wrong type of error for mixed data, which prevented the test cleanup from running and left behind files that triggered further errors. These mistakes reveal a pattern of models following natural wording or common defaults rather than the exact format shown in the example, and they also highlight a weakness in the prompt itself, which was not explicit about key naming or which version of standard deviation to use, while the hidden tests enforced those details rigidly.",Formatting Mistakes
BigCodeBench/424,"The task is to read an image, segment it into color-based regions using K-means clustering, save each region as a separate image, and return both the original and segmented images. All models tripped over a spec–test mismatch and, for some, a missed edge case. The prompt/docs say the image “assumes RGB,” but the tests and canonical solution implicitly expect OpenCV’s raw output (BGR) and compare exact values, so any color-space change makes the arrays fail strict equality; this explains the normal-case failures for deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, and sonnet-4. A second issue is the single-cluster rule: the tests require returning the original image unchanged when n_clusters=1; gpt-4o, llama-3-3, and sonnet-4 didn’t special-case this and therefore also failed that scenario, while deepseek-v3, mistral-3-2, and qwen3-coder handled it. File-saving differences didn’t matter (the tests don’t check them). In short, the chief cause is a confusing prompt (“RGB”) versus tests that demand untouched OpenCV data, compounded by some models not honoring the single-cluster “return original” behavior.",Formatting Mistakes
BigCodeBench/443,"The task is to multiply a matrix with a 3D tensor, flatten the outcome, apply KMeans clustering, and visualize the clustered data. Across models, the core failure was a shared misunderstanding of what “flatten the result” meant relative to the hidden reference pipeline and tests: instead of producing a specific samples-by-3 layout and a single scatter of all points, several models collapsed everything to one dimension or kept the wrong sample count, so their clustering and plots no longer matched the test expectations. deepseek-v3, llama-3-3, and qwen3-coder flattened to a single feature, which often yielded only 2 clusters and too many plotted points; gpt-4o formed too few samples (then crashed when clusters exceeded samples); mistral-3-2 used a transposed layout that changed the sample count; and sonnet-4 drew multiple scatters, so the test—expecting one collection—counted only a subset of points. Many also failed the strict label-equality check (“bool has no attribute all”) because their preprocessing diverged from the canonical one, and several ignored the fixed tensor shape. A contributing issue is that the prompt is a bit vague (“flatten the result”) while the tests impose a very specific interpretation and plotting style, creating an easy mismatch between reasonable implementations and the exact requirements.","Flawed/Incomplete Algorithm, Wrong Problem Mapping, Formatting Mistakes"
BigCodeBench/458,"The task is to take a JSON string, double all its numeric values, and return the result as a pandas DataFrame. Across models, most failures came from two misunderstandings: how to build the DataFrame when lists are present and how to treat numeric-looking strings. gpt-4o, llama-3-3, and qwen3-coder flattened everything into a single row, ignoring that list columns imply multiple rows with scalars broadcast to match—hence widespread shape mismatches and the wrong result for the empty dict case. deepseek-v3 and mistral-3-2 instead tried to build a table from all-scalar dictionaries and crashed; mistral-3-2 also tripped on a nested dict inside a list by trying to “regex” non-strings. For numeric strings, llama-3-3 and mistral-3-2 extracted digits inside any text (over-eager), while qwen3-coder and sonnet-4 edited strings or forced floats, producing values like “10.0” where the tests expect integer 10. Overall, the prompt’s line “numerical values stored as floats” and the phrase “strings that can be interpreted as numbers” may have nudged models toward over-conversion, but the tests and reference logic clearly expect only fully numeric strings to become numbers, broadcasting when lists exist, and preserving integer results when appropriate.","Flawed/Incomplete Algorithm, Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/486,"The task is to generate and plot a time series between two epoch times with a given step, random variation, and a linear trend. The task is to generate and plot a time series between two epoch times with a given step, random variation, and a linear trend. Across models, failures stemmed from three simple misreads: missing basic input checks, misinterpreting time units, and not matching the test’s brittle labeling expectation. deepseek-v3, gpt-4o, qwen3-coder, and sonnet-4 all generated reasonable-looking plots but skipped the “start > end must raise” guard and relied on auto-formatted datetime ticks, so the year “1970” didn’t reliably appear as the test expects. llama-3-3 veered off-spec by treating inputs like datetimes/timedeltas, which let it run but still miss the label expectation and validation. mistral-3-2 changed the interface entirely (expecting ISO dates and pandas frequency strings), clashing with the integer millisecond inputs in the tests and failing early. A contributing factor is the prompt/docs saying epoch milliseconds while several models implicitly assumed seconds; combined with a test (case 8) that specifically looks for “1970” on tick labels, this created avoidable mismatches. Overall, the models behaved sensibly at a glance, but small contract and formatting assumptions—unseen by them at generation time—caused systematic, predictable failures.","Wrong Problem Mapping, Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/492,"The task is to generate daily random sales data for five products starting from a given epoch time up to the current time. Across models, failures mostly came from three mismatches with the tests rather than from core generation logic. First, several models (deepseek-v3, qwen3-coder, sonnet-4) interpreted the prompt’s phrasing “between the date of the given epoch time” literally and normalized timestamps to midnight, while the tests require preserving the exact start timestamp—including time-of-day—so their first row didn’t match. Second, others (gpt-4o, llama-3-3, deepseek-v3) enforced “epoch must be an integer,” but the tests pass floating-point millisecond values, causing premature errors. Third, many (gpt-4o, deepseek-v3, mistral-3-2, qwen3-coder, sonnet-4; llama-3-3 partly) didn’t fully validate that products are exactly five unique items, and some (gpt-4o, llama-3-3, mistral-3-2) skipped the future-time check, leading to missed exceptions. A minor oddity is the duplicate stub-then-final function some models emitted, which didn’t cause the failures but shows uncertain drafting. Overall, the main driver is ambiguity between “date” (suggesting day-level) and the tests’ expectation of exact datetimes, plus a mismatch between the prompt’s “int” wording and test inputs, which set traps that several models—each in slightly different ways—fell into.","Flawed/Incomplete Algorithm, Edge Case Mishandling"
BigCodeBench/501,"The task is to take a JSON string and convert it into an Excel .xls file, correctly handling empty arrays and returning the file path. Across the board, the models mostly failed because they wrote Excel via pandas’ writers in an environment that didn’t have the required engines for .xls, while the task and tests clearly lean on xlwt (and the canonical solution avoids pandas writers entirely). gpt-4o and mistral-3-2 hard-picked the xlsxwriter route, which wasn’t installed and also clashes with the .xls expectation; llama-3-3 and qwen3-coder relied on pandas auto-selection, which couldn’t find any engine for .xls; sonnet-4 mixed approaches—using xlwt only for empty sheets but pandas for non-empty—so it still hit the missing-engine wall; and deepseek-v3 added a filesystem misstep (trying to create a directory from an empty path) that blocked all tests. The prompt and tests aren’t faulty, but they implicitly expect direct xlwt usage; most models overlooked that cue, chose a pandas-writer approach, and ran into a predictable environment mismatch rather than a logic error.","Flawed/Incomplete Algorithm, Wrong Problem Mapping"
BigCodeBench/503,"The task is to generate a pandas DataFrame of random stock prices for a given number of past days, ending with today. Across models, most failures stem from two broad issues: missing input checks and fragile date handling. deepseek-v3, gpt-4o, mistral-3-2, and sonnet-4 all skipped validation, so they didn’t raise the expected errors for bad inputs; deepseek-v3 and gpt-4o (also mistral-3-2) further used time-stamped dates, so tiny timing differences made supposedly identical runs disagree. llama-3-3 miscounted the required days, producing one extra row (showing up even more clearly on the leap-year test), while qwen3-coder built the date index in a way the tests didn’t expect, triggering attribute errors. A small spec inconsistency likely added confusion: the prompt says prices should be in [0,1), but the reference/example suggests larger values—fortunately, tests only check that values are numeric. Overall, the tests strongly assume stable, day-based dates and explicit validation; when models (like sonnet-4) handled dates at the day level they were fine on reproducibility, but most failed by overlooking these simple contract details.","Flawed/Incomplete Algorithm, Edge Case Mishandling"
BigCodeBench/509,"The task is to compare two CSV files and generate a report in a DataFrame showing line-by-line differences with their status and content. Across models, failures stemmed from treating CSVs as plain text rather than structured rows, which skewed the reported content and introduced spurious “?” diff hints; this led to mismatched outputs versus the example that clearly implies row-wise comparison and tuple-like content. Several systems (e.g., deepseek-v3, sonnet-4, mistral-3-2) also wrapped the required empty-file ValueError into a generic Exception, breaking the tests’ expectations. Line numbering was another common pitfall: many (gpt-4o, qwen3-coder, llama-3-3) tracked per-file line numbers or produced duplicates/gaps instead of the simple sequential positions the tests expect. Overall, the prompt’s phrasing “line number in the file” is a bit ambiguous and likely nudged models toward per-file counting, while the provided example and canonical behavior actually expect the diff sequence index; that mismatch, plus inconsistent error propagation, explains the widespread divergence.","Wrong Problem Mapping, Edge Case Mishandling"
BigCodeBench/511,"The task is to analyze employee data by computing basic statistics for a chosen column and visualizing the results with a pie chart labeled by ages. All models stumbled because they implicitly treated the employee data as records with named fields instead of the specified list-of-lists [Age, Salary, Experience], so their DataFrames didn’t align with the required column names and everything downstream broke. Deepseek-v3, Llama-3-3, Mistral-3-2, and Qwen3-coder hit immediate missing-column errors; GPT-4o silently fell back to default stats (never computing anything) and failed to raise the required error for an invalid column; Sonnet-4 enforced the wrong error type and aborted even on valid inputs. Several also drifted from the intended pie-chart behavior (e.g., GPT-4o and Mistral-3-2 plotting age counts; Qwen3-coder altering label text), which would have failed the label/slice checks even if earlier issues hadn’t stopped them. A few responses were noisy (duplicate function stubs, extra examples), hinting at prompt-following instability. The tests are strict but consistent with the spec; the only friction point is that the natural “employee data” phrasing nudged models toward a dict-based assumption, while the doc_struct clearly fixes the format—missing that detail explains the cascade of failures.","Wrong Problem Mapping, Edge Case Mishandling"
BigCodeBench/526,"The task is to read a JSON file of dictionaries and create a Pandas DataFrame showing the mean and median for each key. Across all models (deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4), the core failure was a shared misunderstanding of the task’s intent: they treated “ignore non-numeric or missing values” as a reason to exclude keys entirely if those keys never had a numeric value, instead of keeping every key in the union and yielding NaN statistics when appropriate. This led to missing rows in the index (e.g., key “b” in mixed or missing cases) and, in fully non-numeric inputs, to empty DataFrames that triggered KeyErrors. The tests and doc text clearly required including all keys and returning NaN when no numeric data exist, so the issue lies with model behavior rather than ambiguous prompts or flawed tests. Minor quirks appeared—deepseek-v3, gpt-4o, and others emitted duplicate function stubs, and sonnet-4 tried to fabricate sample data on file errors—but these were incidental; the principal cause of failure was not constructing results for keys lacking numeric entries, which broke expectations about the sorted union index and NaN outputs.","Flawed/Incomplete Algorithm, Edge Case Mishandling"
BigCodeBench/528,"The task is to read a CSV file, detect duplicate rows, and visualize their counts in a bar chart. Across models, failures largely stem from small but strict test expectations rather than core logic: most systems (deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4) produced sensible duplicate detection and plotting, but used human-friendly plot titles (e.g., “Duplicate Rows…”) instead of the exact “Duplicate Entries,” and several (notably deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4) returned an Axes even when no duplicates existed, while the tests require ax to be None then. Some approaches added extra “helpfulness” that backfired against the tests: mistral-3-2 and sonnet-4 converted missing-file errors into ValueError (the tests expect FileNotFoundError), and multiple models drew empty “no duplicates” charts, which the tests reject. The prompt is generally clear, but the tests impose hidden, exact UI strings and ax behavior not emphasized in the description, creating a mismatch: models that aimed for better UX or broader error handling were penalized by strict assertions rather than faulty duplicate logic.",Formatting Mistakes
BigCodeBench/553,"The task is to generate a random pandas DataFrame based on two input lists and plot it as a bar chart, always returning a valid plot object. Across all models (deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, qwen3-coder, sonnet-4), the core failure was the same: they happily handled the “normal” case but didn’t account for empty inputs, so when either a or b (or both) were empty, they tried to plot an empty DataFrame and crashed with pandas errors; the tests, however, implicitly require returning a valid (even blank) Axes in those cases. This gap stems partly from the prompt, which doesn’t spell out the empty-input behavior, while the tests and canonical solution enforce it—so the spec is a bit under-specified and the tests are stricter than the plain reading. Secondary quirks also appeared: gpt-4o added an unnecessary hard error when b exceeds the predefined names (stricter than required), and qwen3-coder/sonnet-4 didn’t clamp column names, risking a mismatch if b were longer (not hit by the given tests). Several responses duplicated stubs or included extra example code (e.g., deepseek-v3, gpt-4o), suggesting prompt-literalism rather than careful edge-case thinking. In short, the models implemented the happy path but overlooked the test-driven contract around empty inputs, which is what actually caused the failures.",Edge Case Mishandling
BigCodeBench/560,"The task is to parse a yearly data string and plot a bar chart of monthly values, returning the corresponding matplotlib Axes object. In this task, all models failed mainly because they misunderstood the required input format and validation rules defined by the prompt and tests. The function was supposed to take a simple string of ""yyyy-mm-value"" entries, but DeepSeek-v3 assumed a dictionary, GPT-4o, Mistral-3-2, and Qwen3-coder expected a DataFrame, Llama-3-3 enforced a strict 12-month list, and Sonnet-4 treated the input as tuples of dates and values. These mismatches caused type errors or incorrect behavior in every case. Several models also used the current year for the chart title instead of extracting it from the data, and most skipped the required error checks for empty input or mixed years. Llama-3-3 in particular over-constrained the data by demanding exactly twelve months, while Sonnet-4 returned an empty plot instead of raising an error. Overall, the failures reflect a strong bias toward common data-science conventions (dicts, DataFrames, fixed monthly data) rather than following the exact contract given in the prompt and tests.","Wrong Problem Mapping, Edge Case Mishandling"
BigCodeBench/567,"The task is to create a function that takes a dash-separated string of numbers and draws a histogram showing their frequency distribution. Across models, the failures mostly stem from misreading the input format and overlooking what the tests actually assert. The task (and doc_struct) clearly says the numbers are dash-separated, yet deepseek-v3, gpt-4o, llama-3-3, mistral-3-2, and qwen3-coder all assumed spaces/commas, causing immediate parse errors on strings like “1-2-3.” Even when parsing didn’t crash (e.g., the single value “7”), most models relied on Matplotlib’s default ticks, so auto-generated decimal ticks didn’t match the tests’ expectation of ticks exactly equal to the unique data values; this explains the recurring “[6.4, 6.6, …, 7.6] vs [7]” assertions. Sonnet-4 went further off by using a regex that treated hyphens as minus signs, flipping values negative and producing obviously wrong tick ranges. A minor oddity is that several responses included duplicate function definitions, but that didn’t drive the failures. One prompt/test misalignment likely contributed: while bins and labels are specified, the requirement to set x-ticks to the discrete data values is only implied by the tests and the canonical solution, not stated explicitly—making it easy for otherwise reasonable plotting code to fail strict tick checks.",Formatting Mistakes
BigCodeBench/579,"The task is to read text from a CSV file, normalize it to ASCII, count word frequencies, and return the top 10 words along with a bar chart. Across models, two broad issues drove failures: environment-triggered plotting crashes and a hidden case-sensitivity mismatch. deepseek-v3, gpt-4o, qwen3-coder, and sonnet-4 added extra plotting layout steps that tripped a font/layout error in the grading environment, so their code failed before logical checks; deepseek-v3 also assumed text was only in the first CSV column, which would undercount. llama-3-3 and mistral-3-2 avoided the plotting crash but treated counting as case-insensitive, so “Café” normalized to “cafe,” conflicting with the tests’ expectation of “Cafe.” Several answers also showed generation artifacts (duplicate stubs or stray lines), hinting at unstable completions but not primary failure causes. A prompt/test misalignment contributed: the instructions never specify case rules, yet the tests implicitly require preserving case after ASCII normalization, and they are sensitive to environment-fragile plotting choices.","Flawed/Incomplete Algorithm, Edge Case Mishandling"
BigCodeBench/582,"The task is to generate normally distributed random numbers, plot their histogram with a probability density function, and return the resulting figure. In this task, all models produced visually reasonable solutions but failed the tests for the same underlying reason: they hard-coded the number of histogram bins instead of matching the test’s hidden requirement of using automatic bin calculation. Deepseek-v3, GPT-4o, Llama-3-3, Mistral-3-2, and Sonnet-4 all defaulted to 30 bins, while Qwen3-coder used 50, leading to mismatches with the expected 28 bins. Some models, like Llama-3-3 and Mistral-3-2, also introduced unnecessary randomness seeding, but this did not affect the outcome. The key issue is not that the models misunderstood the prompt—which did not specify how to choose bins—but that the test cases enforced a stricter interpretation based on “auto” binning, creating a mismatch between what seemed correct to the models and what the evaluation demanded. This highlights how hidden test constraints, rather than poor model reasoning, primarily drove the failures.",Edge Case Mishandling
BigCodeBench/583,"The task is to generate an RSA key pair, encrypt the private key with AES, save it to a uniquely named file, and return the public key along with encryption details. Across this task, all models showed consistent struggles because the tests required very specific conditions that were not clearly emphasized in the prompt—most importantly that the RSA key must be 512 bits, the AES mode must be EAX, and the encrypted private key must be stored as base64 text rather than raw binary or other formats. Deepseek-v3 and GPT-4o defaulted to 2048-bit keys and used AES-GCM, which immediately broke compatibility with the tests. Llama-3-3 and Mistral-3-2 both ran into simple type-handling mistakes that caused their code to crash outright, on top of also using the wrong key size and output format. Qwen3-coder attempted a more elaborate approach by writing PEM-like headers and extra information, but this clashed with the test’s expectation of a plain base64 string. Sonnet-4 came closest in structure, using the right AES mode, but still failed because it included unnecessary data in the file and also defaulted to 2048-bit keys. Overall, the failures show a common pattern: the models followed general cryptography habits (using larger keys, adding authentication data, or structuring files) rather than matching the very strict and somewhat hidden requirements in the tests, leading to systematic mismatches.","Flawed/Incomplete Algorithm, Formatting Mistakes"
BigCodeBench/592,"The task is to generate simulated sensor data for a given number of hours and save it into a CSV file with the required columns. Across all models, the main source of failure came from not following the strict expectations of the tests, often due to small but important misunderstandings. Several models such as deepseek-v3, qwen3-coder, and sonnet-4 chose to generate timestamped filenames instead of the fixed one the tests required, which caused the file not to be found. Others like llama-3-3 and mistral-3-2 misinterpreted “generate data for hours” as producing per-minute values, leading to far too many rows. Some, including gpt-4o, simply printed the output instead of returning it, or returned the wrong path, again failing the tests. These issues were reinforced by ambiguities in the prompt and documentation: the description of the return value conflicted with the tests, and the wording about generating data “for hours” was open to different interpretations. Overall, the models often produced reasonable-looking solutions from a general coding perspective, but small deviations from the precise contract defined by the tests led to consistent failures.","Wrong Problem Mapping, Formatting Mistakes"
BigCodeBench/593,"The task is to generate random traffic data for different vehicle types over time, save it to a CSV file, and plot the results on a line chart. All models missed the test suite’s very specific contract, which went beyond the natural reading of the prompt, so otherwise “reasonable” choices caused failures. Several (mistral-3-2, qwen3-coder, sonnet-4) wrote files with timestamped names and created directories in a more robust way, but the tests demanded one fixed path and an exact call pattern. Others (deepseek-v3, gpt-4o) skipped the read-back step the tests explicitly check for, and some (llama-3-3, deepseek-v3, qwen3-coder) didn’t trigger the expected plotting call. The common edge case—zero hours—was also mishandled: instead of treating it as “no plot,” many still produced a chart or even errored. Overall, these failures reflect a mismatch between pragmatic coding habits and brittle test expectations; the prompt itself didn’t clearly state those strict behaviors (fixed filename, enforced read-back, mandatory plot display, special zero-hour handling), so models that followed sensible approaches still failed the tightly specified tests.","Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/594,"The task is to generate weather data for a given number of hours, save it as a CSV file, and create a backup copy in a designated folder. Across all models, failures mainly stem from drifting from the task’s very specific contract rather than from generating CSV rows themselves. The tests expect a fixed file name and a backup to a precise directory, plus an explicit check that this backup folder exists; many models ignored or reinterpreted these constraints. deepseek-v3, llama-3-3, and mistral-3-2 favored “more robust” behaviors (like inventing timestamped names or different copy methods), which broke the strict expectations. gpt-4o changed the backup destination to a different place and name, while qwen3-coder put the backup under the output folder but still didn’t follow the exact checking and call pattern the tests look for. sonnet-4 was closest on paths yet still didn’t match the expected verification and copy behavior. In short, the models optimized for general practicality, but the prompt and tests demand exact paths, names, and a particular sequence of actions; that mismatch—rather than logic errors—caused most of the failures.","Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/618,"The task is to generate a small football results table for fixed teams with random goals and penalty fines, and create two simple plots showing these values. Across models, the main failure came from not matching the strict output schema the tests expect: a 5-row table (one per listed team) with exactly the columns “Team,” “Goals,” and “Penalty Cost.” Several models—deepseek-v3, llama-3-3, qwen3-coder, and sonnet-4—added a raw “Penalties” column, making the shape wrong; gpt-4o also generated 10 rows instead of one per team; and mistral-3-2 diverged furthest by building a home/away match format, so the required columns weren’t present at all. These choices suggest a common reading: “penalties are converted into fines” was taken to mean “include both penalties and their converted fines,” which is reasonable in everyday analysis but conflicts with the tests’ exact column list. Minor quirks like duplicated headers appeared but weren’t the cause. In short, small schema mismatches—amplified by slightly ambiguous wording in the prompt versus very strict tests—explain nearly all failures.","Wrong Problem Mapping, Edge Case Mishandling"
BigCodeBench/636,"The task is to create a DataFrame with random numbers, count non-zero values in each column, and show these counts in a bar plot. All models, including Deepseek-v3, GPT-4o, Llama-3-3, Mistral-3-2, Qwen3-coder, and Sonnet-4, produced reasonable solutions for the main task of generating data and plotting counts, but they consistently failed on the edge cases. The failures occurred because none of them handled situations where the number of rows was zero or negative: instead of returning empty results as the tests required, they either produced empty bars on the chart or triggered errors. This shows that while the models correctly addressed the “normal” scenario, they did not anticipate hidden expectations in the tests, which were not clearly stated in the prompt. Some models also added unnecessary stubs or stylistic differences, but the critical issue was a mismatch between the test design and the prompt, leading to uniform failure across systems.",Edge Case Mishandling
BigCodeBench/654,"The task is to fit an exponential decay curve to the row indices of a 2D array where the first column matches a given target value. Across models, most failures stem from misunderstanding the task’s intent—fit an exponential to the indices where the first column matches—combined with weak edge-case handling enforced by the tests. deepseek-v3, llama-3-3, and mistral-3-2 incorrectly tried to fit values from another column, which in the provided tests are strings, leading to type issues and non-convergent fits; they also tripped on arrays with only one column. gpt-4o similarly hit an index error on the “not enough points” case. qwen3-coder took the right general approach (using indices) but didn’t guard against too few matches and suffered convergence failures. sonnet-4 avoided crashes but violated the requirement to raise a ValueError for insufficient data. Overall, the prompt is clear but easy to misread (“fit to the indices”), and the tests intentionally use string data and a strict “<3 points → ValueError” rule to expose such misreads and missing checks, while also implicitly requiring robust fitting (increased evaluation budget) to pass.","Wrong Problem Mapping, Edge Case Mishandling"
BigCodeBench/655,"The task is to extract topics from a collection of text documents using preprocessing, TF-IDF vectorization, and Non-Negative Matrix Factorization (NMF). Across models, most failures stemmed from a mismatch between the prompt’s stated output and the tests’ hidden expectations, plus weak handling of edge cases. The prompt says to return a list of topic-word lists, but the tests sometimes expect a tuple with an extra None on empty or stopword-only input and a raised error for zero topics—requirements not signposted in the instruction. As a result, gpt-4o, llama-3-3, and mistral-3-2 crashed when texts were empty or reduced to only stopwords; they didn’t guard for “no vocabulary” scenarios. deepseek-v3 additionally broke the tests by altering the global stopword object in a way the tests couldn’t slice. qwen3-coder tweaked vectorizer settings that backfired on a single document, causing an error unrelated to topic extraction itself. sonnet-4 tried to be defensive but returned an empty list instead of raising the required error and didn’t follow the tuple return the tests expect. In short, the main logic was generally fine, but small spec gaps between the prompt and the test contract, plus brittle choices around edge inputs, led to systematic failures.","Flawed/Incomplete Algorithm, Edge Case Mishandling"
BigCodeBench/657,"The task is to clean a list of texts, remove stopwords, and train a Word2Vec model on the processed data. Across models, the main reason for failure was a mismatch between what the tests expected and what the models produced. Most models, including Deepseek-v3, Llama-3-3, Mistral-3-2, Qwen3-Coder, and Sonnet-4, failed because they did not handle the case of empty input texts, which the tests required to return a valid Word2Vec object without training. GPT-4o failed for a different reason: it changed the function’s parameter name, so the tests could not call it correctly. Some models also showed risky habits, like naming conflicts (Deepseek-v3, Llama-3-3) or undefined defaults (Qwen3-Coder), that did not always break in the given tests but reflect unstable reasoning. Overall, the failures highlight how the prompt’s instructions and the hidden test cases place strict requirements on small details—such as exact parameter names and handling of edge cases—that the models often overlooked.","Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/720,"The task is to create a function that generates random temperature and humidity data, writes it to a CSV file, and returns the file path. All models failed on this task mainly because they followed the prompt’s instruction to delete the file after use, while the hidden tests required the file to remain for validation, creating a direct conflict between specification and evaluation. This mismatch explains why every system—whether DeepSeek, GPT-4o, LLaMA-3, Mistral, Qwen, or Sonnet—produced the same file-not-found errors during testing. Beyond this common issue, models also showed secondary inconsistencies: some (like GPT-4o, Mistral, and Sonnet) generated multiple rows instead of one, others (such as DeepSeek, LLaMA-3, Qwen, and Sonnet) used data ranges outside what the tests expected, and Qwen even returned an absolute rather than relative file path. These behaviors show that while the systems attempted to satisfy the ambiguous natural-language prompt, the underlying contradiction between the prompt and the test cases guaranteed failure regardless of their different approaches.","Wrong Problem Mapping, Formatting Mistakes"
BigCodeBench/723,"The task is to scrape tabular data from a webpage and save it into a CSV file. Across models, most failures came from not matching the task’s hidden contract enforced by the tests: scrape the specific data-table table, return the CSV path, and raise errors when no table or URL is invalid. deepseek-v3 tripped over how it fetched/parsed the page and then tried to scrape generic paragraphs instead of the target table. gpt-4o assumed a headerized table and handled “no table” gently rather than erroring, leading to crashes or silent exits. llama-3-3 and mistral-3-2 both wrapped everything in broad “be safe” logic (status checks or catch-all exceptions) that prevented the expected exceptions and even the write from happening. qwen3-coder and sonnet-4 pursued flexible, fallback-heavy scraping (links, headings, generic text), which looks robust in the wild but diverges from the benchmark’s narrow expectations. A test-suite quirk also contributed: it’s interaction-driven and strict, so small deviations in how files are opened or how CSVs are written cause failures even when the overall idea is reasonable. In short, the models generally favored generic, resilient scraping patterns, but the benchmark rewards precise adherence to a very specific behavior contract.",Flawed/Incomplete Algorithm
BigCodeBench/771,"The task is to process CSV files in a directory by matching their names to a pattern and creating new copies with simplified filenames. Across models, failures mostly stem from mismatches with the test harness rather than core CSV copying logic. Several (deepseek-v3, sonnet-4, mistral-3-2, qwen3-coder) avoided os.listdir and instead scanned the filesystem differently, so the tests’ mocked directory contents were never seen, leading to empty outputs and missed error propagation. Others added guardrails that short-circuited the intended behavior: gpt-4o returned early on missing directories (so the mocked file error was never raised), while mistral-3-2 and qwen3-coder raised a different exception than the test expected. Some also deviated from the required outputs—deepseek-v3 returned full paths, and gpt-4o/sonnet-4 changed filenames (e.g., adding a suffix)—and llama-3-3 tripped over how the tests verify file opens. The prompt is mostly clear about outcomes, but it doesn’t hint that the tests depend on specific I/O choices (using os.listdir, returning bare filenames, and matching the test’s call expectations), which made “reasonable” alternative approaches brittle under this test setup.","Formatting Mistakes, Edge Case Mishandling"
BigCodeBench/785,"The task is to create a function that archives files matching a given pattern and deletes the originals, returning the result. In this task, all models failed mainly because of a mismatch between the written instructions and the hidden test expectations. The prompt suggested always returning an archive file path, but the tests actually required returning a fixed message string when no files matched. Models like deepseek-v3, gpt-4o, and mistral-3-2 chose to raise errors in that situation, while llama-3-3, qwen3-coder, and sonnet-4 created and returned empty archive files instead; both approaches contradicted the tests, which expected a specific string and no archive. This inconsistency led to systematic failures across different scenarios, especially when no matching files existed or when the archive directory had been removed. The issue highlights how unclear prompts and test designs that rely on hidden requirements can easily mislead models, even when their reasoning seems consistent with the given instructions.",Edge Case Mishandling
BigCodeBench/800,"The task is to read goal and penalty counts from a CSV file, add new values provided as input, and return the total as a Counter object. Across models, failures mostly stem from misreading the inputs and mishandling the file: several (gpt-4o, sonnet-4, mistral-3-2, qwen3-coder) treated the goals and penalties dictionaries as single numbers, which caused type errors; deepseek-v3 read the CSV without checking if it exists, leading to file-not-found crashes; and llama-3-3 returned None when the file was missing, violating the expected Counter return. A few approaches also tried to “update” the CSV content (deepseek-v3, qwen3-coder), which wasn’t required and distracted from the simple counting task. The prompt likely contributed to confusion: the phrase “update it with the given goals and penalties” and the starter signature without type hints can make models assume scalar inputs, while tests quietly rely on file-absence tolerance and even mix Counter vs plain dict equality (e.g., test 4), adding mild inconsistency. In short, most errors reflect a shallow read of the spec (dict inputs, optional CSV) and unnecessary side effects, rather than complexity of the task itself.","Flawed/Incomplete Algorithm, Edge Case Mishandling"
BigCodeBench/82,"The task is to build a simple Flask app with user authentication using Flask-Login, including login, logout, and a protected page. Across models, failures mostly stem from drifting from the task’s narrow contract rather than core Flask-Login usage: several systems tried to “improve” the app and broke the tests’ assumptions. deepseek-v3 changed route names/redirect targets and omitted the required /protected endpoint, yielding 404s; gpt-4o and llama-3-3 rendered a non-existent protected template, causing errors instead of returning the simple text the tests look for; mistral-3-2 compounded that with template setup issues, making even the login page fail; qwen3-coder rendered a protected template that exposed the test’s mocked user object, so the expected plain message didn’t appear; and sonnet-4 ended up returning no app at all due to a truncated redefinition. A common behavioral pattern was adding extra UX (templates, dashboards, flashes) or duplicating definitions, which conflicted with the test harness, whose logic mocks authentication and expects a very specific string response. The prompt’s mention of “template rendering” may have nudged models to overuse templates—while the tests only provide login.html and require /protected to return plain text—creating a subtle prompt/test mismatch that helped trigger these errors.","Flawed/Incomplete Algorithm, Wrong Problem Mapping"
BigCodeBench/857,"The task is to move files from one folder to another based on given file extensions, while reporting any files that could not be transferred. Across models, failures stem from two themes: wording mismatches against strict tests and a misunderstanding of what “transfer” means here. deepseek-v3, gpt-4o, and mistral-3-2 mostly followed the intended flow but failed because their warning messages didn’t include the exact phrase the tests look for (“Unable to move file”), so even correct behavior read as wrong. llama-3-3, qwen3-coder, and sonnet-4 took a “copy”-style approach instead of “move,” which, combined with how the tests repeatedly feed the same mocked file list, produced duplicate “transfers” and non-empty results even when an error was forced—hence multiple assertion failures. The prompt’s wording (“Transfer files”) is ambiguous and likely encouraged the copy-instead-of-move interpretations, while the test suite is brittle in demanding an exact warning substring and in its mocking setup that magnifies duplicates when models loop per extension. Overall, the models weren’t wildly off; they were tripped by ambiguous phrasing and tight, phrase-sensitive checks.","Wrong Problem Mapping, Formatting Mistakes"
BigCodeBench/89,"The task is to detect and remove outliers from a dataset column using Z-scores, then return the cleaned data along with visual plots for comparison. Overall, the failures stem from two main misreads of the task’s constraints: a subtle return-type contract and the expected data type. Deepseek-v3, GPT-4o, and Qwen3-coder broadly followed the idea (standardize, score, filter, plot) but returned the outlier indices as an array instead of the tuple the tests require—an easy trap given the prompt’s wording versus the test’s strict check. Llama-3-3, Mistral-3-2, and Sonnet-4 assumed a pandas/DataFrame workflow (e.g., dropping rows by index), but the task and tests use plain NumPy arrays, so they crashed on attribute errors before doing real work. Plotting wasn’t the issue; titles and calling show() were satisfied or never reached. The prompt and doc_struct are mostly clear, but the tests’ insistence that the third return value be a tuple (not just “indices”) is a gotcha that likely caused the otherwise reasonable array return from several models.","Flawed/Incomplete Algorithm, Formatting Mistakes"
BigCodeBench/914,"This task required implementing a function to predict future stock prices from a given DataFrame and return both predictions and a plot. The main challenge was handling the correct column names and formatting of the input data. Models like deepseek-v3 and mistral-3-2 assumed the DataFrame had ""Date"" and ""Close"" columns, while gpt-4o and qwen3-coder expected ""timestamp"" and ""close"", leading to mismatches and runtime errors. Similarly, llama-3-3 and sonnet-4 also failed because they tried to access columns that did not exist in the provided test data. Ultimately, all models failed because they made inconsistent assumptions about the schema of the input DataFrame, instead of adapting flexibly to the actual column names used in the test cases.",Flawed/Incomplete Algorithm
BigCodeBench/915,"The task involves identifying and flagging outliers in a DataFrame column using Z-scores, and extending this to multiple columns. Specifically, the user wanted to flag outliers in col_D first, and then simultaneously in col_D and col_E. Several models, including Deepseek-v3 and GPT-4o, attempted to solve the problem by calculating Z-scores and highlighting outliers, but their implementations consistently failed when accessing a non-existent 'Z_score' column in the test cases. Similarly, LLaMA-3-3 and Mistral-3-2 correctly computed outliers but did not maintain the intermediate Z-score column required for testing, causing all test cases that referenced 'Z_score' to break. The main cause of failure across all models was the mismatch between the expected presence of a 'Z_score' column in the test cases and the models’ output, which either did not include this column or named it differently.",Formatting Mistakes
BigCodeBench/916,"The task required implementing a function to visualize stock closing prices using both a box plot and a histogram. The main challenge was ensuring compatibility with the provided test cases, which used a column named closing_price instead of the expected Close. Several models failed due to rigid assumptions about the column name: DeepSeek, Mistral, Qwen, and Sonnet directly accessed the Close column, leading to KeyError exceptions when the test input used closing_price. GPT-4o and Llama-3 introduced column validation, but still hard-coded Close, which caused them to raise errors rather than adapt. Ultimately, all models failed on this task because they did not handle the input schema flexibly, instead relying on a fixed column name.",Flawed/Incomplete Algorithm
BigCodeBench/917,"The problem is about implementing a function that forecasts the next seven days of stock closing prices using the ARIMA model and plots the results. The function needed to correctly handle the input DataFrame and produce both numerical forecasts and a visualization. However, DeepSeek, GPT-4o, and LLaMA-3.3 assumed a strict schema (requiring a Date column or misusing it), which caused repeated column key errors. Similarly, Mistral, Qwen, and Sonnet models relied on a Close column that was not guaranteed in all test cases, leading to crashes. Ultimately, all models failed because they rigidly assumed a fixed input structure instead of implementing robust handling of varying DataFrame formats.",Flawed/Incomplete Algorithm
BigCodeBench/93,"The problem requires implementing Principal Component Analysis (PCA) on a dataset, returning both the transformed data as a DataFrame and a scatter plot of the first two components. It must also raise a ValueError if n_components is not a positive integer. The expected input is a DataFrame and an integer n_components, and the output should be a (DataFrame, Axes) pair. Despite producing syntactically correct solutions, models produced close but mismatched values, leading to assertion errors. The main cause of failure across all models was the omission of the fixed random seed (np.random.seed(42)), which the canonical solution used to ensure deterministic PCA results for testing.",Edge Case Mishandling
BigCodeBench/942,"The task requires generating and visualizing a sales report across multiple product categories over time, ensuring both the data and the visualization match expected specifications. The expected inputs include a start date, number of periods, frequency, and category list, and the function must output a DataFrame with {Date, Category, Sales} and a Matplotlib Axes object. However, when tested, all LLM-generated solutions failed because their plots used titles such as ""Sales Report by Category Over Time"" or ""Sales Report"" instead of the canonical ""Category-wise Sales Trends."" Comparing the canonical solution with LLM outputs shows the main cause of failure was not logic or data generation but strict string mismatches in plot titles.",Formatting Mistakes
BigCodeBench/945,"The task requires generating a time series of sales data starting from a specified date and using linear regression to forecast future sales. The problem is challenging because the forecasted values must reflect realistic trends without blindly increasing, even when the input sales are strictly increasing. The function expects inputs including a start date, the number of periods, a frequency string, and optionally an array of sales data, and it should output a numpy array containing forecasted sales for the same number of periods. All evaluated models produced forecasts that consistently failed the test_forecasted_values_increasing, indicating that their predictions were monotonically increasing when the test expected more nuanced behavior. Comparison with the canonical solution shows that the LLMs correctly implemented data generation and regression fitting, but none accounted for the subtle test requirement that forecasted trends should not simply follow an increasing pattern. The primary cause of failure is that all models used standard linear regression without incorporating mechanisms to handle non-monotonic patterns or seasonality, which is essential to pass the specific trend-based test.",Flawed/Incomplete Algorithm
BigCodeBench/955,"The task requires replacing spaces in specified phrases with underscores in a given text, then plotting the frequency in a case-insensitive manner. The function must raise a ValueError if the input text is empty. Inputs are a list of target phrases (mystrings) and a text string, while the output is a matplotlib.axes.Axes object showing the frequency distribution. Across all LLM-generated solutions, models failed to handle phrase-level replacements correctly and often applied naive space replacements or tokenization, which broke expected outputs. For example, in the case of ""hello world → hello_world!"", models frequently dropped punctuation or split the phrase into ""hello"" and ""world!"" instead of preserving it as a single unit. The main reason for failure is that the models did not integrate phrase-level substitution with proper word boundary and punctuation handling, leading to mismatches against the canonical solution.",Flawed/Incomplete Algorithm
BigCodeBench/964,"This task demands writing a function that converts files with extensions .txt, .docx, .xlsx, and .csv from a source directory into CSV files stored in a target directory. The problem also requires raising a FileNotFoundError if the source directory does not exist and returning the count of successfully converted files. The inputs are the paths of the source and target directories, and the output is an integer representing the number of converted files. However, across models such as GPT-4o, DeepSeek-v3, Llama-3-3, Mistral-3-2, Qwen3-Coder, and Sonnet-4, none passed the tests, with all failing test case 9 that checked conversion of files inside nested subdirectories. Comparing their code to the canonical solution shows that every model only iterated over the top-level files in the source directory without recursively walking through subfolders. Thus, the main cause of failure is the consistent omission of recursive directory traversal (os.walk) that the canonical implementation correctly uses.",Edge Case Mishandling
BigCodeBench/985,"The task requires generating a population report as both a Pandas DataFrame and a CSV file from JSON input while enforcing strict validation rules on the data. The expected input is a JSON string containing a ""Countries"" dictionary mapping country names to populations, and the output is a (file_path, DataFrame) tuple where the CSV is written without an index column. All models failed by assuming alternative JSON formats different from the required JSON schema.",Wrong Problem Mapping
BigCodeBench/988,"The task requires evaluating each file and directory in a given path against specific conditions like is_file, is_dir, has_special_chars, and has_numbers, raising exceptions for invalid predicates or nonexistent directories. The function should take a directory path and a list of predicates as input and return a dictionary mapping item names to dictionaries of predicate results. The models failed either by misinterpreting the inputs or wrongly structuring the outputs.",Flawed/Incomplete Algorithm
