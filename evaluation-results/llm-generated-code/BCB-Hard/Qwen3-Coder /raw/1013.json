{
  "task_id": "BigCodeBench/1013",
  "instruct_prompt": "This function scrapes a webpage for all hyperlinks and saves them as absolute URLs to a CSV file.\nThe function should output with:\n    int: The number of unique absolute links scraped from the webpage.\nYou should write self-contained code starting with:\n```\nimport requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n```",
  "llm_response": "import requests\nfrom urllib.parse import urljoin\nfrom bs4 import BeautifulSoup\nimport csv\ndef task_func(\n    url: str,\n    base_url: str = \"https://www.example.com\",\n    csv_file: str = \"scraped_data.csv\",\n) -> int:\n    # Send a GET request to the URL\n    response = requests.get(url)\n    \n    # Parse the HTML content using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find all anchor tags with href attributes\n    links = soup.find_all('a', href=True)\n    \n    # Set to store unique absolute URLs\n    absolute_urls = set()\n    \n    # Convert links to absolute URLs and add to set\n    for link in links:\n        href = link['href']\n        absolute_url = urljoin(base_url, href)\n        absolute_urls.add(absolute_url)\n    \n    # Write the absolute URLs to a CSV file\n    with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n        writer = csv.writer(file)\n        writer.writerow(['url'])  # Write header\n        for absolute_url in sorted(absolute_urls):\n            writer.writerow([absolute_url])\n    \n    # Return the number of unique absolute links\n    return len(absolute_urls)",
  "model": "qwen3-coder"
}